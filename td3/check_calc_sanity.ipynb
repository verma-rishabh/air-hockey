{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorboard_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"/run/media/luke/Data/uni/SS2023/DL Lab/Project/qualifying/DDPG_exp2/replay/data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_files': ['state.npy',\n",
       "  'action.npy',\n",
       "  'next_state.npy',\n",
       "  'reward.npy',\n",
       "  'not_done.npy'],\n",
       " 'files': ['state', 'action', 'next_state', 'reward', 'not_done'],\n",
       " 'allow_pickle': False,\n",
       " 'max_header_size': 10000,\n",
       " 'pickle_kwargs': {'encoding': 'ASCII', 'fix_imports': True},\n",
       " 'zip': <zipfile.ZipFile file=<_io.BufferedReader name='/run/media/luke/Data/uni/SS2023/DL Lab/Project/qualifying/DDPG_exp2/replay/data_4actions.npz'> mode='r'>,\n",
       " 'f': <numpy.lib.npyio.BagObj at 0x7f84cc380760>,\n",
       " 'fid': <_io.BufferedReader name='/run/media/luke/Data/uni/SS2023/DL Lab/Project/qualifying/DDPG_exp2/replay/data_4actions.npz'>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = x[\"state\"]\n",
    "next_state = x[\"next_state\"]\n",
    "reward = x[\"reward\"]\n",
    "not_done = x[\"not_done\"]\n",
    "action = x[\"action\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105000, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9745588709802906"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(action[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 105000\n",
    "\t\tself.size = 105000\n",
    "\n",
    "\t\tself.state = state\n",
    "\t\tself.action = action\n",
    "\t\tself.next_state = next_state\n",
    "\t\tself.reward = reward\n",
    "\t\tself.not_done = not_done\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "\t\t)\n",
    "\t\n",
    "\tdef save(self,filename):\n",
    "\t\tnp.savez(filename,state = self.state[:self.size,::],action = self.action[:self.size,::],\\\n",
    "\t\t\tnext_state = self.next_state[:self.size,::],reward = self.reward[:self.size,::],not_done =self.not_done[:self.size,::])\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from air_hockey_challenge.framework.air_hockey_challenge_wrapper import AirHockeyChallengeWrapper\n",
    "from air_hockey_agent.agent_builder_ddpg_exp2_hit import build_agent\n",
    "# from air_hockey_challenge.environments.planar.hit import AirHockeyHit\n",
    "from tensorboard_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AirHockeyChallengeWrapper(env=\"7dof-hit\", interpolation_order=3, debug=False)\n",
    "# policy = build_agent(env.env_info)\n",
    "policy = build_agent(env.env_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(23, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "from air_hockey_challenge.utils.kinematics import inverse_kinematics, jacobian\n",
    "i=1120\n",
    "state, done = env.reset(), False\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base [0.41601167 0.94092308 0.00703506 0.16203352 0.03662689 0.52028456]\n",
      "policy [-0.15598626  0.23548583 -0.03901886 -0.16438065  0.22543091 -0.04693529]\n"
     ]
    }
   ],
   "source": [
    "# state = replay_buffer.state[i,:]\n",
    "action_ = replay_buffer.action[i,:]\n",
    "# print(action)\n",
    "action = np.zeros((2,7))\n",
    "des_pos = [action_[0],action_[1],0.1645]\n",
    "# print(des_pos)\n",
    "\n",
    "_,x = inverse_kinematics(policy.robot_model, policy.robot_data,des_pos)\n",
    "action[0,:] = x\n",
    "des_v = action_[2:]\n",
    "jac = jacobian(policy.robot_model, policy.robot_data,policy.get_joint_pos(state))\n",
    "inv_jac = np.linalg.pinv(jac)\n",
    "joint_vel = des_v@inv_jac.T[:3,:]\n",
    "action[1,:] = joint_vel\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "state = next_state\n",
    "env.render()\n",
    "print(\"base\",env.base_env.get_ee()[1])\n",
    "print(\"policy\",(jac@policy.get_joint_vel(state))[:])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = \"/run/media/luke/Data/uni/SS2023/DL Lab/Project/qualifying/DDPG_exp2\"\n",
    "tensorboard_dir=main_dir + \"/tensorboard/\"\n",
    "tensorboard = Evaluation(tensorboard_dir, \"train\", [\"critic_loss\",\"actor_loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(15000):\n",
    "    critic_loss=np.nan\n",
    "    actor_loss=np.nan\n",
    "    critic_loss,actor_loss = policy.train(replay_buffer,batch_size= 512)\n",
    "    tensorboard.write_episode_data(epoch, eval_dict={ \"critic_loss\" : critic_loss,\\\n",
    "                \"actor_loss\":actor_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save(main_dir + f\"/models/DDPG-v0_air-hockey_0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('challenge')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6100d8334917db35c4ec7cf716c3100bfc66eb35e85e153ba7e378d404aaa54d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
